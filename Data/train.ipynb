{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 一、环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1 安装bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.2 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import chardet\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from html.parser import HTMLParser\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.data import Stack, Pad, Tuple\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from visualdl import LogWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、导入数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.1 定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SelfDefinedDataset(paddle.io.Dataset):\n",
    "    def __init__(self, data):\n",
    "        super(SelfDefinedDataset, self).__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def get_labels(self):\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "def txt_to_list(file_name):\n",
    "    res_list = []\n",
    "    for line in open(file_name):\n",
    "        res_list.append(line.strip().split('\\t'))\n",
    "    return res_list\n",
    "\n",
    "trainlst = txt_to_list('train_list.txt')\n",
    "devlst = txt_to_list('eval_list.txt')\n",
    "testlst = txt_to_list('test_list.txt')\n",
    "\n",
    "train_ds, dev_ds, test_ds= SelfDefinedDataset(trainlst), SelfDefinedDataset(devlst), SelfDefinedDataset(testlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 准备标签\n",
    "label_list = train_ds.get_labels()\n",
    "print(label_list)\n",
    "#看看数据长什么样子，分别打印训练集、验证集、测试集的第1条数据。\n",
    "print(\"训练集数据：{}\\n\".format(train_ds[0:1]))\n",
    "print(\"验证集数据:{}\\n\".format(dev_ds[0:1]))\n",
    "print(\"测试集数据:{}\\n\".format(test_ds[0:1]))\n",
    "\n",
    "print(\"训练集样本个数:{}\".format(len(train_ds)))\n",
    "print(\"验证集样本个数:{}\".format(len(dev_ds)))\n",
    "print(\"测试集样本个数:{}\".format(len(test_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#调用ppnlp.transformers.BertTokenizer进行数据处理，tokenizer可以把原始输入文本转化成模型model可接受的输入数据格式。\n",
    "tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "count = 0 \n",
    "#数据预处理\n",
    "def convert_example(example,tokenizer,label_list,max_seq_length=256,is_test=False):\n",
    "    if is_test:\n",
    "        text = example\n",
    "    else:\n",
    "        text, label = example\n",
    "    #tokenizer.encode方法能够完成切分token，映射token ID以及拼接特殊token\n",
    "    encoded_inputs = tokenizer.encode(text=text, max_seq_len=max_seq_length)\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\n",
    "    #注意，在早前的PaddleNLP版本中，token_type_ids叫做segment_ids\n",
    "    segment_ids = encoded_inputs[\"token_type_ids\"]\n",
    "    global count\n",
    "    count += 1\n",
    "    if count % 1000 == 0:\n",
    "        print(str(count) + \"\\r\")\n",
    "\n",
    "    if not is_test:\n",
    "        label_map = {}\n",
    "        for (i, l) in enumerate(label_list):\n",
    "            label_map[l] = i\n",
    "\n",
    "        label = label_map[label]\n",
    "        label = np.array([label], dtype=\"int64\")\n",
    "        return input_ids, segment_ids, label\n",
    "    else:\n",
    "        return input_ids, segment_ids\n",
    "\n",
    "#数据迭代器构造方法\n",
    "def create_dataloader(dataset, trans_fn=None, mode='train', batch_size=1, use_gpu=False, pad_token_id=0, batchify_fn=None):\n",
    "    if trans_fn:\n",
    "        # dataset = dataset.apply(trans_fn, lazy=True)\n",
    "        dataset = SelfDefinedDataset(list(map(trans_fn, dataset)))\n",
    "\n",
    "    if mode == 'train' and use_gpu:\n",
    "        sampler = paddle.io.DistributedBatchSampler(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "    else:\n",
    "        shuffle = True if mode == 'train' else False #如果不是训练集，则不打乱顺序\n",
    "        sampler = paddle.io.BatchSampler(dataset=dataset, batch_size=batch_size, shuffle=shuffle) #生成一个取样器\n",
    "    dataloader = paddle.io.DataLoader(dataset, batch_sampler=sampler, return_list=True, collate_fn=batchify_fn)\n",
    "    return dataloader\n",
    "\n",
    "#使用partial()来固定convert_example函数的tokenizer, label_list, max_seq_length, is_test等参数值\n",
    "trans_fn = partial(convert_example, tokenizer=tokenizer, label_list=label_list, max_seq_length=128, is_test=False)\n",
    "batchify_fn = lambda samples, fn=Tuple(Pad(axis=0,pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id), Stack(dtype=\"int64\")):[data for data in fn(samples)]\n",
    "#训练集迭代器\n",
    "train_loader = create_dataloader(train_ds, mode='train', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\n",
    "#验证集迭代器\n",
    "dev_loader = create_dataloader(dev_ds, mode='dev', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)\n",
    "#测试集迭代器\n",
    "test_loader = create_dataloader(test_ds, mode='test', batch_size=64, batchify_fn=batchify_fn, trans_fn=trans_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.1 加载预训练模型Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载预训练模型Bert用于文本分类任务的Fine-tune网络BertForSequenceClassification, 它在BERT模型后接了一个全连接层进行分类。\n",
    "#由于本任务中的恶意网页识别是二分类问题，设定num_classes为2\n",
    "model = ppnlp.transformers.BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#设置训练超参数\n",
    "\n",
    "#学习率\n",
    "learning_rate = 5e-5\n",
    "#训练轮次\n",
    "epochs = 10\n",
    "#学习率预热比率\n",
    "warmup_proption = 0.1\n",
    "#权重衰减系数\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "num_warmup_steps = int(warmup_proption * num_training_steps)\n",
    "\n",
    "def get_lr_factor(current_step):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    else:\n",
    "        return max(0.0,\n",
    "                    float(num_training_steps - current_step) /\n",
    "                    float(max(1, num_training_steps - num_warmup_steps)))\n",
    "#学习率调度器\n",
    "lr_scheduler = paddle.optimizer.lr.LambdaDecay(learning_rate, lr_lambda=lambda current_step: get_lr_factor(current_step))\n",
    "\n",
    "#优化器\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ])\n",
    "\n",
    "#损失函数\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "#评估函数\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#评估函数，设置返回值，便于VisualDL记录\n",
    "def evaluate(model, criterion, metric, data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses = []\n",
    "    for batch in data_loader:\n",
    "        input_ids, segment_ids, labels = batch\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.numpy())\n",
    "        correct = metric.compute(logits, labels)\n",
    "        metric.update(correct)\n",
    "        accu = metric.accumulate()\n",
    "    print(\"eval loss: %.5f, accu: %.5f\" % (np.mean(losses), accu))\n",
    "    model.train()\n",
    "    metric.reset()\n",
    "    return np.mean(losses), accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#开始训练\n",
    "global_step = 0\n",
    "max_acc = 0\n",
    "with LogWriter(logdir=\"./log\") as writer:\n",
    "    for epoch in range(1, epochs + 1):    \n",
    "        for step, batch in enumerate(train_loader, start=1): #从训练数据迭代器中取数据\n",
    "            input_ids, segment_ids, labels = batch\n",
    "            logits = model(input_ids, segment_ids)\n",
    "            loss = criterion(logits, labels) #计算损失\n",
    "            probs = F.softmax(logits, axis=1)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 100 == 0 :\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %.5f, acc: %.5f\" % (global_step, epoch, step, loss, acc))\n",
    "                #记录训练过程\n",
    "                writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\n",
    "                writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_gradients()\n",
    "        eval_loss, eval_acc = evaluate(model, criterion, metric, dev_loader)\n",
    "        #记录评估过程\n",
    "        writer.add_scalar(tag=\"eval/loss\", step=epoch, value=eval_loss)\n",
    "        writer.add_scalar(tag=\"eval/acc\", step=epoch, value=eval_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if eval_loss>max_acc:\n",
    "            max_acc = eval_loss\n",
    "            print('saving the best_model...')\n",
    "            paddle.save(model.state_dict(), 'best_model')\n",
    "# 保存最终模型\n",
    "paddle.save(model.state_dict(),'final_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 保存模型和网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to static graph with specific input description\n",
    "model = paddle.jit.to_static(\n",
    "    model,\n",
    "    input_spec=[\n",
    "        paddle.static.InputSpec(\n",
    "            shape=[None, None], dtype=\"int64\"),  # input_ids\n",
    "        paddle.static.InputSpec(\n",
    "            shape=[None, None], dtype=\"int64\")  # segment_ids\n",
    "    ])\n",
    "# Save in static graph model.\n",
    "paddle.jit.save(model, './static_graph_params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![](https://ai-studio-static-online.cdn.bcebos.com/ba3a959e6fac4bbf8309cebd06b42ee05185739543444eb6b5e5eb97d5237985)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<font size=4>可以看到，使用BERT预训练模型进行finetune，10个epoch内验证集准确率已经可以达到96.7%以上。</font>\n",
    "\n",
    "<font size=4>在VisualDL中，也可以查看BERT模型的网络结构，方法如下：</font>\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/e344c72f506642bb9bceedd28eadbcd40f16c1afaf31468b8e9b420e2c7ed305)\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/abd6f73a222645c29979c451b0970d36e47b9ef6c0044e6caca04f58ddc7c52b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、预测效果\n",
    "\n",
    "<font size=5>完成上面的模型训练之后，可以得到一个能够通过HTML标签序列识别是否存在恶意网页的模型。接下来查看模型在测试集上的表现，非常接近97%。<font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval loss: 0.10088, accu: 0.96998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10087659, 0.9699795081967213)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 评估模型在测试集上的表现\n",
    "evaluate(model, criterion, metric, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict(model, data, tokenizer, label_map, batch_size=1):\n",
    "    examples = []\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(text, tokenizer, label_list=label_map.values(),  max_seq_length=128, is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(Pad(axis=0, pad_val=tokenizer.pad_token_id), Pad(axis=0, pad_val=tokenizer.pad_token_id)): fn(samples)\n",
    "    batches = []\n",
    "    one_batch = []\n",
    "    for example in examples:\n",
    "        one_batch.append(example)\n",
    "        if len(one_batch) == batch_size:\n",
    "            batches.append(one_batch)\n",
    "            one_batch = []\n",
    "    if one_batch:\n",
    "        batches.append(one_batch)\n",
    "\n",
    "    results = []\n",
    "    model.eval()\n",
    "    for batch in batches:\n",
    "        input_ids, segment_ids = batchify_fn(batch)\n",
    "        input_ids = paddle.to_tensor(input_ids)\n",
    "        segment_ids = paddle.to_tensor(segment_ids)\n",
    "        logits = model(input_ids, segment_ids)\n",
    "        probs = F.softmax(logits, axis=1)\n",
    "        idx = paddle.argmax(probs, axis=1).numpy()\n",
    "        idx = idx.tolist()\n",
    "        labels = [label_map[i] for i in idx]\n",
    "        results.extend(labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# 获取某知名网站首页链接\n",
    "r = requests.get(\"https://www.csdn.net\")\n",
    "demo = r.text\n",
    "soup=BeautifulSoup(demo,\"html.parser\")\n",
    "tags = []\n",
    "for tag in soup.find_all(True):\n",
    "\ttags.append(tag.name)\n",
    "data = []\n",
    "data.append(','.join(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测网页: https://www.csdn.net \n",
      "网页标签: 正常网页\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: '恶意网页', 1: '正常网页'}\n",
    "\n",
    "predictions = predict(model, data, tokenizer, label_map, batch_size=64)\n",
    "for idx, text in enumerate(data):\n",
    "    print('预测网页: {} \\n网页标签: {}'.format(\"https://www.csdn.net\", predictions[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 小结\n",
    "- <font size=3>在该项目中，进一步完善了HTML标签序列的提取流程，项目也优化了日志文件写入方式，在ViusalDL中训练过程将连续显示。</font>\n",
    "- <font size=3>使用BERT预训练模型Finetune后，两种分类模型预测准确率已经接近97%。</font>\n",
    "- <font size=3>在项目最后，已经出现了网页识别工程化的雏形（获取网页链接——提取标签序列——判断网页类型），接下来将进一步探索。</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
